{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70eb535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bb7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "Training:    Loss: 5.6637 | Acc: 66.94%\n",
      "Validation:  Loss: 0.9089 | Acc: 81.48%\n",
      "             Precision: 81.41%\n",
      "             Recall: 81.48%\n",
      "             F1-score: 81.43%\n",
      "\n",
      "Epoch 2/100\n",
      "Training:    Loss: 2.3096 | Acc: 85.95%\n",
      "Validation:  Loss: 0.7604 | Acc: 82.72%\n",
      "             Precision: 82.63%\n",
      "             Recall: 82.72%\n",
      "             F1-score: 82.62%\n",
      "\n",
      "Epoch 3/100\n",
      "Training:    Loss: 1.6555 | Acc: 84.30%\n",
      "Validation:  Loss: 1.1905 | Acc: 88.89%\n",
      "             Precision: 90.64%\n",
      "             Recall: 88.89%\n",
      "             F1-score: 88.49%\n",
      "\n",
      "Epoch 4/100\n",
      "Training:    Loss: 0.7445 | Acc: 92.15%\n",
      "Validation:  Loss: 1.2964 | Acc: 87.65%\n",
      "             Precision: 88.20%\n",
      "             Recall: 87.65%\n",
      "             F1-score: 87.40%\n",
      "\n",
      "Epoch 5/100\n",
      "Training:    Loss: 0.7172 | Acc: 91.74%\n",
      "Validation:  Loss: 0.6990 | Acc: 87.65%\n",
      "             Precision: 87.63%\n",
      "             Recall: 87.65%\n",
      "             F1-score: 87.59%\n",
      "\n",
      "Epoch 6/100\n",
      "Training:    Loss: 0.6747 | Acc: 91.32%\n",
      "Validation:  Loss: 0.5131 | Acc: 91.36%\n",
      "             Precision: 92.46%\n",
      "             Recall: 91.36%\n",
      "             F1-score: 91.14%\n",
      "\n",
      "Epoch 7/100\n",
      "Training:    Loss: 0.4832 | Acc: 90.91%\n",
      "Validation:  Loss: 0.6014 | Acc: 91.36%\n",
      "             Precision: 92.46%\n",
      "             Recall: 91.36%\n",
      "             F1-score: 91.14%\n",
      "\n",
      "Epoch 8/100\n",
      "Training:    Loss: 0.6123 | Acc: 93.80%\n",
      "Validation:  Loss: 0.7727 | Acc: 91.36%\n",
      "             Precision: 92.46%\n",
      "             Recall: 91.36%\n",
      "             F1-score: 91.14%\n",
      "\n",
      "Epoch 9/100\n",
      "Training:    Loss: 0.4711 | Acc: 92.56%\n",
      "Validation:  Loss: 0.6504 | Acc: 91.36%\n",
      "             Precision: 92.46%\n",
      "             Recall: 91.36%\n",
      "             F1-score: 91.14%\n",
      "\n",
      "Epoch 10/100\n",
      "Training:    Loss: 0.3497 | Acc: 92.56%\n",
      "Validation:  Loss: 0.5830 | Acc: 91.36%\n",
      "             Precision: 92.46%\n",
      "             Recall: 91.36%\n",
      "             F1-score: 91.14%\n",
      "\n",
      "Epoch 11/100\n",
      "Training:    Loss: 0.2695 | Acc: 95.04%\n",
      "Validation:  Loss: 0.8502 | Acc: 91.36%\n",
      "             Precision: 92.46%\n",
      "             Recall: 91.36%\n",
      "             F1-score: 91.14%\n",
      "\n",
      "Early stopping triggered! Loading best model...\n",
      "\n",
      "============================================================\n",
      "Final Test Set Performance:\n",
      "============================================================\n",
      "Accuracy:       91.46%\n",
      "Precision:      92.53%\n",
      "Recall:         91.46%\n",
      "F1-score:       91.24%\n",
      "\n",
      "Class-wise Performance:\n",
      "\n",
      "Mastercard:\n",
      "  Accuracy:    78.79%\n",
      "  Precision:   100.00%\n",
      "  Recall:      78.79%\n",
      "  F1:          88.14%\n",
      "  Samples:     33\n",
      "============================================================\n",
      "\n",
      "Others:\n",
      "  Accuracy:    100.00%\n",
      "  Precision:   87.50%\n",
      "  Recall:      100.00%\n",
      "  F1:          93.33%\n",
      "  Samples:     49\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0.0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, val_accuracy, model):\n",
    "        improved = False\n",
    "\n",
    "        if val_loss < (self.best_val_loss - self.min_delta):\n",
    "            self.best_val_loss = val_loss\n",
    "            improved = True\n",
    "        if val_accuracy > (self.best_val_acc + self.min_delta):\n",
    "            self.best_val_acc = val_accuracy\n",
    "            improved = True\n",
    "\n",
    "        if improved:\n",
    "            self.counter = 0\n",
    "            self.best_model_state = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate multiple classification metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': np.mean(y_true == y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted')\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def calculate_class_metrics(y_true, y_pred, classes):\n",
    "    \"\"\"Calculate metrics per class\"\"\"\n",
    "    results = {}\n",
    "    for i, class_name in enumerate(classes):\n",
    "        true_positives = np.sum((y_true == i) & (y_pred == i))\n",
    "        support = np.sum(y_true == i)\n",
    "        \n",
    "        results[class_name] = {\n",
    "            'true_positives': true_positives,\n",
    "            'precision': precision_score(y_true == i, y_pred == i, zero_division=0),\n",
    "            'recall': recall_score(y_true == i, y_pred == i, zero_division=0),\n",
    "            'f1': f1_score(y_true == i, y_pred == i, zero_division=0),\n",
    "            'support': support\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Path to dataset and transformations\n",
    "data_dir = 'Mastercard dataset'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load and split dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "indices = list(range(len(dataset)))\n",
    "labels = [label for _, label in dataset]\n",
    "\n",
    "# Stratified train/val/test split (60/20/20)\n",
    "train_idx, temp_idx = [], []\n",
    "for label in np.unique(labels):\n",
    "    label_idx = np.where(np.array(labels) == label)[0]\n",
    "    np.random.shuffle(label_idx)\n",
    "    split = int(0.6 * len(label_idx))\n",
    "    train_idx.extend(label_idx[:split])\n",
    "    temp_idx.extend(label_idx[split:])\n",
    "\n",
    "val_idx, test_idx = [], []\n",
    "for label in np.unique(labels):\n",
    "    label_idx = [i for i in temp_idx if labels[i] == label]\n",
    "    np.random.shuffle(label_idx)\n",
    "    split = int(0.5 * len(label_idx))\n",
    "    val_idx.extend(label_idx[:split])\n",
    "    test_idx.extend(label_idx[split:])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                         sampler=SubsetRandomSampler(train_idx), num_workers=2)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                       sampler=SubsetRandomSampler(val_idx), num_workers=2)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        sampler=SubsetRandomSampler(test_idx), num_workers=2)\n",
    "\n",
    "# Model definition\n",
    "class LogoClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogoClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogoClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    train_accuracy = correct / total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_metrics = calculate_metrics(np.array(val_labels), np.array(val_preds))\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'Training:':<12} Loss: {avg_train_loss:.4f} | Acc: {100*train_accuracy:.2f}%\")\n",
    "    print(f\"{'Validation:':<12} Loss: {avg_val_loss:.4f} | Acc: {100*val_accuracy:.2f}%\")\n",
    "    print(f\"{'':<12} Precision: {100*val_metrics['precision']:.2f}%\")\n",
    "    print(f\"{'':<12} Recall: {100*val_metrics['recall']:.2f}%\")\n",
    "    print(f\"{'':<12} F1-score: {100*val_metrics['f1']:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping(avg_val_loss, val_accuracy, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"\\nEarly stopping triggered! Loading best model...\")\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "        break\n",
    "\n",
    "# Final test evaluation\n",
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_metrics = calculate_metrics(np.array(test_labels), np.array(test_preds))\n",
    "class_metrics = calculate_class_metrics(np.array(test_labels), np.array(test_preds), dataset.classes)\n",
    "\n",
    "# Print test results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Test Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Accuracy:':<15} {100*test_metrics['accuracy']:.2f}%\")\n",
    "print(f\"{'Precision:':<15} {100*test_metrics['precision']:.2f}%\")\n",
    "print(f\"{'Recall:':<15} {100*test_metrics['recall']:.2f}%\")\n",
    "print(f\"{'F1-score:':<15} {100*test_metrics['f1']:.2f}%\")\n",
    "print(\"\\nClass-wise Performance:\")\n",
    "for class_name, metrics in class_metrics.items():\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  {'Accuracy:':<12} {100*(metrics['true_positives']/metrics['support'] if metrics['support'] > 0 else 0):.2f}%\")\n",
    "    print(f\"  {'Precision:':<12} {100*metrics['precision']:.2f}%\")\n",
    "    print(f\"  {'Recall:':<12} {100*metrics['recall']:.2f}%\")\n",
    "    print(f\"  {'F1:':<12} {100*metrics['f1']:.2f}%\")\n",
    "    print(f\"  {'Samples:':<12} {metrics['support']}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Save model and prediction function\n",
    "torch.save(model.state_dict(), 'logo_classifier.pth')\n",
    "\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "    return dataset.classes[predicted.item()], probabilities[0].cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
